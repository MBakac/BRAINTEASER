In this section we will discuss the different approaches to asking multiple-choice questions and describe the training pipeline.
The source code is available on GitHub\footnote[3]{\url{https://github.com/MBakac/BRAINTEASER}}.
\subsection{Reframing multiple choice}
This section will discuss the main point of this paper -- different formulations of a multiple-choice question for BERT.
We will first describe the already explored multiclass and binary classification approaches and introduce the pairwise classification approach.

\subsubsection{Multiclass classification}
For the multiclass classification approach, the question and answers are concatenated and the problem is framed as multiclass classification where the classes are answers $A$, $B$, $C$, and $D$ (None of the above).
The main advantage of this approach is that the model sees all the answers in each pass.

\subsubsection{Binary classification}
In reframing the problem as a binary classification task we take every question and answer pair and label it $1$ if the answer is the correct one, and $0$ otherwise.
When determining the final chosen answer for each question, we group the pairs by question, and, if the model outputs $1$ for only one answer, we chose that answer, otherwise $D$ (None of the above) is chosen.
One advantage of this is that we artificially have more training examples, i.e., question-answer pairs.

\subsubsection{Pairwise approach}
For the new pairwise approach, we transform the questions in the following way.
Each tuple comprised of a question and its candidate answers
$$
    (Q,A,B,C,D)
$$
is transformed into three tuples:
\begin{align*}
       (Q,A,B,D), \\
       (Q,A,C,D), \\
       (Q,B,C,D).
\end{align*}
In case the correct answer was $C$, the ground-truth label in the tuple $(Q,A,B,D)$ is $D$ (None of the above).
For each set of tuples, \emph{votes} for each answer are counted and the answer with the most votes is chosen.
We aim to see if this approach sampling distractors multiple times for each example will yield better results.

\subsection{Training process}
In line with the approach taken by \citep{ails-lab} we load a pre-trained BERT-like model from Hugging Face.
The data evaluation function is modified according to the reformulation and a BERT-like model is fine-tuned to the data.

For this work we decided to use general pre-trained models RoBERTa-large\footnote[4]{\url{https://huggingface.co/FacebookAI/roberta-large}} \citep{roberta} and DeBERTa-v3-base\footnote[5]{\url{https://huggingface.co/microsoft/deberta-v3-base}} \citep{deberta,debertav3} as our goal was to compare different formulations, and was not to achieve the highest score.
For future work, this approach could be taken with less general, fine-tuned BERT-like models.

In our study, we explore binary, pairwise, and multiple classification approaches, utilizing the Hugging Face Transformers library for model fine-tuning. 
For each classification task, we preprocess the data by renaming the column "label" to "labels" and removing irrelevant columns such as "id", "question", "answer", 
and distractors. The models are trained on a dataset tokenized using AutoTokenizer, and set to a format compatible with PyTorch. We employ the
AutoModelForSequenceClassification for binary classification and AutoModelForMultipleChoice for both pairwise and multiple classification tasks. 
The training hyperparameters are set to a batch size of 4, learning rate of 3e-5, and a varying number of epochs: 3 for binary and multiple classification, 
and 1 for pairwise classification. The training process uses the AdamW optimizer and a linear scheduler with no warmup steps. The training arguments include 
evaluating the model every 20 steps, logging every 20 steps, saving checkpoints every 100 steps, and disabling report integrations by setting report\_to=None. 
The models are trained and evaluated on a CUDA-enabled GPU if available. The Trainer class from Hugging Face is employed to manage the training loop, with datasets 
split into training and validation sets, ensuring a structured approach to model training and evaluation. 
