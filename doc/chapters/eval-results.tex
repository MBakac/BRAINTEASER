We compared the three described formulations of multiple choice answering across two BERT variantas and look at the model accuracy overall and specifically for \emph{semantic} and \emph{context} subsets separated for the Sentence puzzle and Word puzzle subtasks (Table \ref{tab:sentence-results-table} and Table \ref{tab:word-results-table}). 
Instance- and group-based performance metrics for our models as well as the human, ChatGPT and RoBERTa-L baselines are presented.
The results are presented in terms of accuracy.
The \emph{Original}, \emph{Semantic} and \emph{Context} columns in the tables represent the results on respective reconstructions in the dataset while \emph{Ori. + Sem} and \emph{Ori. + Sem. + Con.} represent accuracy scores across multiple reconstructions of the same question.

Looking at Table \ref{tab:sentence-results-table}, it is evident that the metrics show that the accuracy is homogenous with regards to the set of questions.
In order words, if an original question is answered correctly, it is likely that the semantically and contextually reconstructed questions will be answered correctly as well.
We take this as a good sign, because it signifies how the models are able to reason about the context behind the questions.

Table \ref{tab:word-results-table} shows that our models exhibit unhomogenous accuracy, or that answering an original question accurately does not make it substantially likelier to answer the semantically and contextually reconstructed questions correctly.
This is because Word puzzles are harder to solve by locating the context behind the answer, as evidenced by ChatGPT's baseline performing worse in the Sentence puzzle subtask.

The calculation for group metrics in the pairwise approach was performed in a characteristic way.
This was done in a matter-of-factly way by implementing a simple voting mechanism.
Since the results are of acceptable quality, the voting system is justified.
However, implementing other voting systems, such as a weighted voting system, which is outside of the scope of this paper, could improve the results in the future.

\paragraph{Context and semnatic reconstruction results} \


\paragraph{Zero accuracy in binary classification formulation} \ % malo lijep≈°e sve to 
A clear outliar in Table \ref{tab:word-results-table} are the accuracy results in in the binarcy classification formulation of multiple choice.
This is due to the \emph{harsh} criteria for deciding on an answer with this approach where out of three binary question only one has to be a \emph{A}, \emph{B} or \emph{C} and the other two $D$ \emph{(None of the above)}.
This, combined with the rather small dataset and, also the word puzzle task being overall harder (our models yielded overall worse accuracy scores for this task) meant that there were no cases where this approach worked, hence accuracy zero.
Furthermore, we note that the models have been trained to have a strong bias towards answering negatively to all instances.
This is partially caused by the fact that the dataset used to train the models for binary classification is imbalanced, with a ratio of 1:3 between positive and negative instances.
This is a significant issue, as the models are not able to learn the patterns of the positive instances, which are the ones that we are interested in finding out.
The same poor results for this formulation were obtained by Panagiotopoulos et. al. \citep{ails-lab}.

As a whole, the results of the study are remarkable in that human accuracy is within reach of our best-performing models by less than 10\%.
Despite the popularity of ChatGPT, it is not the best model for this task.
Our models have managed to the baseline reference paper ChatGPT model by well over 10\% in overall accuracy and by 30\% in group accuracy \citep{semeval}.
This is a significant improvement over the current state-of-the-art models, which is a testament to the power of our approach.
