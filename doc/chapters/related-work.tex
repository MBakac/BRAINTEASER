Reasoning is a hot topic in NLP, with transformer-based models in the center of attention.
It is no secret that transformer-based models are far from perfect in reasoning \citep{yuan2023}, and that it takes just one session of asking questions about a topic that one is well-versed in to see inconsistencies but also to be amazed at time.
Knowing where a model fails is important and may lead to improvement.  Many benchmarks have been performed with the purpose of challenging models with more intricate reasoning, like "Commonsense QA" \citep{commonsenseQA}, a dataset comprised of questions that are easy for humans and that require no prior knowledge such as a specific document or context, and require just common sense.
Similarly, "BRAINTEASER" \citep{semeval} is a benchmark dataset comprised of questions that are constructed in such a way that it is required to consider multiple approaches when answering them.
As this was a competition dataset, there was previous work that tackled this problem.
\citet{ails-lab} showed promising results, significantly outperforming baselines of 60\% reported in \citep{semeval}.
Their approach consisted of fine-tuning models with a straightforward approach, treating the problem as a multiclass classification task. Additionally they performed a transformation of the problem to a binary classification problem. 
The transformation worked by taking each question with four candidate answers and mapping it to three questions that required a binary label, signalling whether the answer was correct or not \footnote[1]{The fourth candidate answer was always "None of the above" so they discarded it.}.
Their results showed that the transformation was not quite useful, as the same models had somewhat worse accuracies when the transformation was applied.  
This inspired us to explore another transformation of the problem, as we believe that presenting a model with only one candidate answer takes away critical information.
When one is answering a question with candidate answers, it is a good idea to eliminate candidates that are obvious outliers.
That is why we believe that an approach where we force a model to choose between two candidate answers at a time might be helpful, and this is described in detail in Section \ref{approach}.